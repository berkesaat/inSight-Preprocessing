{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from glob import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy import signal\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import numpy.random as npr\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class EEGDataset(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, data_dict, split='train', flatten = True):\n",
    "\n",
    "        # --------------------------------------------------------------------------------------------------------\n",
    "        # Prep object and raw data\n",
    "        self.split = split\n",
    "        self.flatten = flatten\n",
    "             \n",
    "        self.eeg, self.emb = data_dict[split]           \n",
    "        # Compute size\n",
    "        self.size = len(self.eeg)\n",
    "    # Get size\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    # Get item\n",
    "    def __getitem__(self, i):\n",
    "        eeg_i = self.eeg[i]\n",
    "        emb_i = self.emb[i]\n",
    "\n",
    "        if self.flatten == True:\n",
    "            eeg_i = eeg_i.flatten()\n",
    "            emb_i = emb_i.flatten()\n",
    "        to_return = (torch.from_numpy(eeg_i), torch.from_numpy(emb_i))\n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "#print(is_cuda)\n",
    "device = torch.device(\"cuda:0\" if is_cuda else \"cpu\")\n",
    "print(device)\n",
    "max_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Linear(ninp, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp*2, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "        self.fc1 = nn.Linear(ninp, ninp*2)\n",
    "        self.fc2 = nn.Linear(ninp*2, ninp)\n",
    "        self.fc3 = nn.Linear(ninp*2, ninp)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features = int(ninp*2))\n",
    "        self.bn2 = nn.BatchNorm1d(num_features = int(ninp))\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    " \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "        \n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "    \n",
    "    \n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        #print(output.shape)\n",
    "        dim0 = output.shape[0]\n",
    "        h1 = self.bn1(self.fc1(output[:,-1,:]))#.view(-1, ninp*2)))\n",
    "        #h1 = h1.view(dim0, -1)\n",
    "        print('h1', h1.shape)\n",
    "        mu, logvar = self.fc2(h1), self.fc3(h1)\n",
    "        \n",
    "        output = self.decoder(self.bn1(h1))#output[:,-1,:]))\n",
    "        return output, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.long).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('finWFT_300_3_o350_s60lf20hf80.pkl', 'rb') as pickle_in:  \n",
    "    dataset = pickle.load(pickle_in) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = nn.Transformer(d_model = 8, nhead = 8, num_encoder_layers = 6).to(device)\n",
    "ntoken, ninp, nhead, nhid, nlayers, dropout = 768, 3400, 2, 200, 2, 0.5\n",
    "model = TransformerModel(ntoken, ninp, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex = torch.ones(10,32,4)\n",
    "#src = Variable(ex, requires_grad = False)\n",
    "#tgt = Variable(ex, requires_grad = False)\n",
    "#ey = model.forward(src, tgt)\n",
    "#src.shape\n",
    "#ey[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33762, 3, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_src = Variable(torch.from_numpy(dataset['train'][0]), requires_grad = False)\\ntrain_tgt = Variable(torch.from_numpy(dataset['train'][0]), requires_grad = False)\\ntest_src = Variable(torch.from_numpy(dataset['test'][0]), requires_grad = False)\\ntest_tgt = Variable(torch.from_numpy(dataset['test'][0]), requires_grad = False)\\ndataset['train'][0].shape, dataset['train'][1].shape\\n\""
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsz = 64\n",
    "train_set = EEGDataset(dataset, split = 'train')\n",
    "train_generator = DataLoader(train_set, batch_size=bsz, shuffle=True)\n",
    "test_set = EEGDataset(dataset, split = 'test')\n",
    "test_generator = DataLoader(test_set, batch_size=bsz, shuffle=True)\n",
    "print(dataset['train'][1].shape)\n",
    "'''\n",
    "train_src = Variable(torch.from_numpy(dataset['train'][0]), requires_grad = False)\n",
    "train_tgt = Variable(torch.from_numpy(dataset['train'][0]), requires_grad = False)\n",
    "test_src = Variable(torch.from_numpy(dataset['test'][0]), requires_grad = False)\n",
    "test_tgt = Variable(torch.from_numpy(dataset['test'][0]), requires_grad = False)\n",
    "dataset['train'][0].shape, dataset['train'][1].shape\n",
    "'''\n",
    "\n",
    "#for i, (eeg, emb) in enumerate(train_generator):\n",
    " #   print(i, eeg.shape, emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kl divergence\n",
    "def loss_function(recon_y, y, mu, logvar):\n",
    "    bce = F.mse_loss(recon_y, y)\n",
    "    print(\"bce\", bce)\n",
    "    kld = -0.5*torch.sum(1+logvar-mu.pow(2)-logvar.exp())\n",
    "    print(\"kld\", kld)\n",
    "    return bce+kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(22.6855, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(55748.3281, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(22.1435, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(444024.5000, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(72.1830, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(3.7554e+16, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(42.6089, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(2.5854e+18, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(93.1621, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(2.9944e+30, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n",
      "bce tensor(nan, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "kld tensor(nan, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "h1 torch.Size([64, 6800])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-398-ff1079fefc69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m#print(recon_batch.shape, y.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m#print(loss, loss.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-396-081a224dc264>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(recon_y, y, mu, logvar)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#kl divergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bce\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mkld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlogvar\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlogvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2202\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2203\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2204\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2205\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    losses = {\"train\": 0, \"test\": 0}\n",
    "    counts = {\"train\": 0, \"test\": 0}\n",
    "    \n",
    "    lr *= 0.5**(epoch//5)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    for data_x, data_y in train_generator:\n",
    "        #print(data_y.shape)\n",
    "        #print(data_x.shape)\n",
    "        \n",
    "        #x2 = torch.stack([data_x for i in range(8)], axis = 2)\n",
    "        #print(x2.shape)\n",
    "        \n",
    "        #x = x2.float().to(device)\n",
    "        x = data_x.float().to(device)\n",
    "        y = data_y.float().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #src = Variable(x, requires_grad = False)\n",
    "        #print(src.size(1))\n",
    "        #tgt = Variable(x, requires_grad = False)\n",
    "        \n",
    "        #output = model(x)\n",
    "        \n",
    "        #y = y.squeeze(1)\n",
    "        \n",
    "        #ndims = bsz*ntoken\n",
    "        #loss = loss_func(output.view(-1, ndims), y) \n",
    "        \n",
    "        recon_batch, mu, logvar = model(x)\n",
    "        \n",
    "        #print(recon_batch.shape, y.shape)\n",
    "        loss = loss_function(recon_batch, y, mu, logvar)\n",
    "        #print(loss, loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses['train'] += loss.item()\n",
    "        counts['train'] += 1\n",
    "        \n",
    "        \n",
    "    \n",
    "    for idx, (data_x, data_y) in enumerate(test_generator):\n",
    "        x = data_x.float().to(device)\n",
    "        y = data_y.float().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(x)\n",
    "        \n",
    "        loss = loss_function(recon_batch, y, mu, logvar)\n",
    "        \n",
    "        losses['test'] += loss.item()\n",
    "        counts['test'] += 1\n",
    "        \n",
    "        \n",
    "    train_losses.append(losses['train']/counts['train'])\n",
    "    test_losses.append(losses['test']/counts['test'])\n",
    "    #print(train_losses, test_losses)\n",
    "    \n",
    "    print(\"Epoch[{}/{}] Training Loss: {}, Test Loss: {}\".format(epoch+1, max_epochs, \n",
    "                                                                 train_losses[-1], test_losses[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
